# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cYycp1IHdAoP4aclcsJ_4S9cphnQm5P_
"""
from flask_ngrok import run_with_ngrok
from flask import Flask, render_template, url_for
from flask_wtf import FlaskForm
from wtforms import StringField, SubmitField, TextAreaField
from wtforms.validators import DataRequired
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

class TextGenerationForm(FlaskForm):
    text = TextAreaField('Text', validators=[DataRequired()])
    length = StringField('Length', validators=[DataRequired()])
    submit = SubmitField('Generate')

tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = GPT2LMHeadModel.from_pretrained('gpt2-medium')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def adjust_length_to_model(length, max_sequence_length):
    if length < 0 and max_sequence_length > 0:
        length = max_sequence_length
    elif 0 < max_sequence_length < length:
        length = max_sequence_length  # No generation bigger than model size
    elif length < 0:
        length = MAX_LENGTH  # avoid infinite loop
    return length

def gpt2_lm(text, length):
  encoded_prompt = tokenizer.encode(text, add_special_tokens=False, return_tensors="pt")
  output_sequences = model.generate(
          input_ids=encoded_prompt,
          max_length=length + len(encoded_prompt[0]),
          temperature=1.0,
          top_k=0,
          top_p=0.9,
          repetition_penalty=1.0,
          do_sample=True,
          num_return_sequences=1,
      )
  # Remove the batch dimension when returning multiple sequences
  if len(output_sequences.shape) > 2:
      output_sequences.squeeze_()

  generated_sequences = []

  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
      print("=== GENERATED SEQUENCE {} ===".format(generated_sequence_idx + 1))
      generated_sequence = generated_sequence.tolist()

      # Decode text
      text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)

      # Remove all text after the stop token
      text = text[: None]

      # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing
      total_sequence = (
          text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]
      )

      generated_sequences.append(total_sequence)
  return total_sequence

app = Flask(__name__, root_path='/content/drive/My Drive/Colab Notebooks/Covid 19 Articles/language-model-app')
app.config['SECRET_KEY'] = '9bad6913d4358ac1395c5c94370ed090'
run_with_ngrok(app)
print(app.root_path)

@app.route("/", methods=['GET', 'POST'])
def home():
  form = TextGenerationForm()
  if form.validate_on_submit():
    text = form.text.data
    length = int(form.length.data)
    generated_sequence = gpt2_lm(text, length)
    return render_template('index.html', form=form, response=generated_sequence)
  return render_template('index.html', form=form)
  
if __name__ == '__main__':
    app.run()

